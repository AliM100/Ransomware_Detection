{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXgZ5xpUHLK3"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOda9uYjQm4R"
      },
      "outputs": [],
      "source": [
        "!git clone 'https://github.com/AliM100/Ransomware_Detection.git'\n",
        "!pip install patool"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/malimg_dataset.zip -d data"
      ],
      "metadata": {
        "id": "mAWa45GiYPGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doyaiCPqHGYt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import os\n",
        "from math import log\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import shutil\n",
        "import pandas as pd\n",
        "import patoolib\n",
        "import seaborn as sns\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from datetime import datetime\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import average_precision_score,accuracy_score\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import tensorflow\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "from Ransomware_Detection.mal_dataset import maldataset\n",
        "from Ransomware_Detection.data_conversion import convert_data\n",
        "from Ransomware_Detection.dataset import load_data,prepare_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvW_hrEwHtqQ"
      },
      "outputs": [],
      "source": [
        "class_index = {'Adialer.C': 0,\n",
        "                'Agent.FYI': 1,\n",
        "                'Allaple.A': 2,\n",
        "                'Allaple.L': 3,\n",
        "                'Alueron.gen!J': 4,\n",
        "                'Autorun.K': 5,\n",
        "                'C2LOP.P': 6,\n",
        "                'C2LOP.gen!g': 7,\n",
        "                'Dialplatform.B': 8,\n",
        "                'Dontovo.A': 9,\n",
        "                'Fakerean': 10,\n",
        "                'Instantaccess': 11,\n",
        "                'Lolyda.AA1': 12,\n",
        "                'Lolyda.AA2': 13,\n",
        "                'Lolyda.AA3': 14,\n",
        "                'Lolyda.AT': 15,\n",
        "                'Malex.gen!J': 16,\n",
        "                'Obfuscator.AD': 17,\n",
        "                'Rbot!gen': 18,\n",
        "                'Skintrim.N': 19,\n",
        "                'Swizzor.gen!E': 20,\n",
        "                'Swizzor.gen!I': 21,\n",
        "                'VB.AT': 22,\n",
        "                'Wintrim.BX': 23,\n",
        "                'Yuner.A': 24}\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0qzB956uonp"
      },
      "outputs": [],
      "source": [
        "data_path=\"data\"\n",
        "img_path=\"data/malimg_paper_dataset_imgs\"\n",
        "data_csvs=\"data/csvs\"\n",
        "save_checkpoints_path=\"data/checkpoint\"\n",
        "batch_size= 10\n",
        "\n",
        "os.makedirs(save_checkpoints_path,exist_ok=True)\n",
        "os.makedirs(data_csvs,exist_ok=True)\n",
        "\n",
        "data_prepare=prepare_data(data_path,img_path,class_index)\n",
        "\n",
        "if not os.path.exists(f\"{data_csvs}/train.csv\"):\n",
        "    data_prepare.create_csv_data()\n",
        "\n",
        "target_size_custom = (224, 224)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(target_size_custom),\n",
        "    # transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.Lambda(lambda x: x.float()),\n",
        "    transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "training_set=maldataset(csv_file=f\"{data_csvs}/train.csv\",root_dir=img_path, class_index=class_index, transform=transform)\n",
        "validation_set=maldataset(csv_file=f\"{data_csvs}/val.csv\",root_dir=img_path, class_index=class_index, transform=transform)\n",
        "\n",
        "training_loader = torch.utils.data.DataLoader(training_set, batch_size=batch_size, shuffle=True)\n",
        "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "classes = ('Adialer.C','Agent.FYI','Allaple.A','Allaple.L','Alueron.gen!J','Autorun.K','C2LOP.P','C2LOP.gen!g','Dialplatform.B','Dontovo.A',\n",
        "           'Fakerean','Instantaccess','Lolyda.AA1','Lolyda.AA2','Lolyda.AA3','Lolyda.AT','Malex.gen!J','Obfuscator.AD','Rbot!gen','Skintrim.N',\n",
        "           'Swizzor.gen!E','Swizzor.gen!I','VB.AT','Wintrim.BX','Yuner.A')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KiETSMxYD_B"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "####best model of the best 97.9 accuracy on malimg 25 classes\n",
        "###dropout 0.25 -> 0.01 accuracy 97.85\n",
        "### single linear layer maybe better choice\n",
        "class CustomModel(nn.Module):\n",
        "    def __init__(self, num_classes=26):\n",
        "        super(CustomModel, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=1, padding=1)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=(2, 2), stride=2)\n",
        "        self.batch_norm1 = nn.BatchNorm2d(64)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(64, 32, kernel_size=(3, 3), stride=1, padding=1)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=(2, 2), stride=2)\n",
        "        self.batch_norm2 = nn.BatchNorm2d(32)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(32, 32, kernel_size=(3, 3), stride=1, padding=1)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=(2, 2), stride=2)\n",
        "        self.batch_norm3 = nn.BatchNorm2d(32)\n",
        "\n",
        "        self.conv4 = nn.Conv2d(32, 16, kernel_size=(3, 3), stride=1, padding=1)\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=(2, 2), stride=2)\n",
        "        self.batch_norm4 = nn.BatchNorm2d(16)\n",
        "\n",
        "        self.conv5 = nn.Conv2d(16, 16, kernel_size=(3, 3), stride=1, padding=1)\n",
        "        self.pool5 = nn.MaxPool2d(kernel_size=(2, 2), stride=2)\n",
        "        self.batch_norm5 = nn.BatchNorm2d(16)\n",
        "\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        self.fc1_input_size = 16 * (target_size_custom[0] // 32) * (target_size_custom[1] // 32)\n",
        "        self.fc1 = nn.Linear(self.fc1_input_size, 256)\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "        self.batch_norm_fc1 = nn.BatchNorm1d(256)\n",
        "\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.dropout2 = nn.Dropout(0.25)\n",
        "        self.batch_norm_fc2 = nn.BatchNorm1d(128)\n",
        "\n",
        "        self.fc3 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.batch_norm1(self.pool1(torch.relu(self.conv1(x))))\n",
        "        x = self.batch_norm2(self.pool2(torch.relu(self.conv2(x))))\n",
        "        x = self.batch_norm3(self.pool3(torch.relu(self.conv3(x))))\n",
        "        x = self.batch_norm4(self.pool4(torch.relu(self.conv4(x))))\n",
        "        x = self.batch_norm5(self.pool5(torch.relu(self.conv5(x))))\n",
        "\n",
        "        x = self.flatten(x)\n",
        "\n",
        "        x = self.batch_norm_fc1(self.dropout1(torch.relu(self.fc1(x))))\n",
        "        x = self.batch_norm_fc2(self.dropout2(torch.relu(self.fc2(x))))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8YAh-e-YD_B"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "######## with dropout ####ORIGINAL\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out += self.shortcut(residual)\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "class CustomModel(nn.Module):\n",
        "    def __init__(self, num_classes=25):\n",
        "        super(CustomModel, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.layer1 = self.make_layer(64, 64, 2)\n",
        "        self.layer2 = self.make_layer(64, 128, 2, stride=2)\n",
        "        self.layer3 = self.make_layer(128, 256, 2, stride=2)\n",
        "        self.layer4 = self.make_layer(256, 512, 2, stride=2)\n",
        "\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "\n",
        "    def make_layer(self, in_channels, out_channels, blocks, stride=1):\n",
        "        layers = []\n",
        "        layers.append(ResidualBlock(in_channels, out_channels, stride))\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(ResidualBlock(out_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7K__WEC8HtqQ"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "\n",
        "model = CustomModel(num_classes=25).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=5e-4)\n",
        "\n",
        "\n",
        "def update_lr(optimizer, lr):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIyfGnkjHtqR"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(epoch_index, tb_writer):\n",
        "    running_loss = 0.\n",
        "    last_loss = 0.\n",
        "    all_true_labels = []\n",
        "    all_predicted_labels = []\n",
        "    # Here, we use enumerate(training_loader) instead of\n",
        "    # iter(training_loader) so that we can track the batch\n",
        "    # index and do some intra-epoch reporting\n",
        "\n",
        "    for i, data in enumerate(training_loader):\n",
        "        # Every data instance is an input + label pair\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        # Zero your gradients for every batch!\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Make predictions for this batch\n",
        "        outputs = model(inputs.float())\n",
        "\n",
        "        # Compute the loss and its gradients\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        # Adjust learning weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # Gather data and report\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        all_true_labels.extend(labels.cpu().numpy())\n",
        "        all_predicted_labels.extend(predicted.cpu().numpy())\n",
        "\n",
        "        if i % 1000 == 999:\n",
        "            last_loss = running_loss / 1000 # loss per batch\n",
        "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
        "            tb_x = epoch_index * len(training_loader) + i + 1\n",
        "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
        "            running_loss = 0.\n",
        "\n",
        "    # scheduler.step(loss)\n",
        "    train_accuracy = accuracy_score(all_true_labels, all_predicted_labels)\n",
        "\n",
        "    return last_loss, train_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxI9-azNHtqR"
      },
      "outputs": [],
      "source": [
        "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
        "checkpoint=\"checkpoints\"\n",
        "os.makedirs(checkpoint,exist_ok=True)\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "writer = SummaryWriter('runs/trainer_{}'.format(timestamp))\n",
        "epoch_number = 0\n",
        "\n",
        "EPOCHS = 100\n",
        "\n",
        "best_vloss = 1_000_000.\n",
        "best_accuracy=0\n",
        "for epoch in range(EPOCHS):\n",
        "    print('EPOCH {}:'.format(epoch_number + 1))\n",
        "\n",
        "    # Make sure gradient tracking is on, and do a pass over the data\n",
        "    model.train(True)\n",
        "    avg_loss, train_accuracy = train_one_epoch(epoch_number, writer)\n",
        "\n",
        "\n",
        "    running_vloss = 0.0\n",
        "    # Set the model to evaluation mode, disabling dropout and using population\n",
        "    # statistics for batch normalization.\n",
        "    model.eval()\n",
        "    val_true_labels = []\n",
        "    val_predicted_labels = []\n",
        "    # Disable gradient computation and reduce memory consumption.\n",
        "    with torch.no_grad():\n",
        "        for i, vdata in enumerate(validation_loader):\n",
        "            vinputs, vlabels = vdata\n",
        "            vinputs, vlabels = vinputs.to(device), vlabels.to(device)\n",
        "            voutputs = model(vinputs.float())\n",
        "            vloss = loss_fn(voutputs, vlabels)\n",
        "            running_vloss += vloss\n",
        "\n",
        "            _, predicted = torch.max(voutputs, 1)\n",
        "            val_true_labels.extend(vlabels.cpu().numpy())\n",
        "            val_predicted_labels.extend(predicted.cpu().numpy())\n",
        "\n",
        "\n",
        "    avg_vloss = running_vloss / (i + 1)\n",
        "    val_accuracy = accuracy_score(val_true_labels, val_predicted_labels)\n",
        "\n",
        "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
        "    print('Accuracy train {} valid {}'.format(train_accuracy, val_accuracy))\n",
        "\n",
        "    # Log the running loss averaged per batch\n",
        "    # for both training and validation\n",
        "    writer.add_scalars('Training vs. Validation Loss',\n",
        "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
        "                    epoch_number + 1)\n",
        "    writer.add_scalars('Training vs. Validation Accuracy',\n",
        "                { 'Training' : train_accuracy, 'Validation' : val_accuracy },\n",
        "                epoch_number + 1)\n",
        "    writer.flush()\n",
        "\n",
        "    # Track best performance, and save the model's state\n",
        "    if val_accuracy > best_accuracy :#or avg_vloss < best_vloss:\n",
        "        #best_vloss = avg_vloss\n",
        "        best_accuracy = val_accuracy\n",
        "        model_path = 'checkpoints/model_{}_{}'.format(timestamp, epoch_number)\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "\n",
        "    epoch_number += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6azjrw9cwVo"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elPnQszxHGYv"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize(target_size_custom),\n",
        "    # transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.Lambda(lambda x: x.float()),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "    ])\n",
        "\n",
        "test_set=maldataset(csv_file=f\"{data_csvs}/test.csv\",root_dir=img_path, class_index=class_index, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "model = CustomModel(num_classes=25).to(device)\n",
        "model.load_state_dict(torch.load(\"checkpoints/*\"), strict=False)\n",
        "\n",
        "running_testloss=0.0\n",
        "model.eval()\n",
        "test_true_labels = []\n",
        "test_predicted_labels = []\n",
        "with torch.no_grad():\n",
        "    for i, data in enumerate(test_loader):\n",
        "        # Every data instance is an input + label pair\n",
        "        inputs, labels = data\n",
        "        inputs, y_gt = inputs.to(device), labels.to(device)\n",
        "\n",
        "        y_test_predicted = model(inputs.float())\n",
        "        test_loss = loss_fn(y_test_predicted, y_gt)\n",
        "        print(test_loss)\n",
        "        running_testloss += test_loss.item()\n",
        "\n",
        "        _, predicted = torch.max(y_test_predicted, 1)\n",
        "        print(predicted)\n",
        "        test_true_labels.extend(labels.cpu().numpy())\n",
        "        test_predicted_labels.extend(predicted.cpu().numpy())\n",
        "\n",
        "    avg_testloss = running_testloss / (i + 1)\n",
        "    test_accuracy = accuracy_score(test_true_labels, test_predicted_labels)\n",
        "\n",
        "    IoU=metrics.jaccard_score(test_true_labels, test_predicted_labels,average=\"micro\")\n",
        "    f1=metrics.f1_score(test_true_labels, test_predicted_labels,average=\"micro\")\n",
        "    print(\"micro IoU\",IoU)\n",
        "    print(\"micro f1\",f1)\n",
        "\n",
        "    IoU=metrics.jaccard_score(test_true_labels, test_predicted_labels,average=\"macro\")\n",
        "    f1=metrics.f1_score(test_true_labels, test_predicted_labels,average=\"macro\")\n",
        "    print(\"macro IoU\",IoU)\n",
        "    print(\"macro f1\",f1)\n",
        "\n",
        "    print('LOSS test{}, Accuracy test{}'.format(avg_testloss,test_accuracy))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXuoVAWMEjAW"
      },
      "outputs": [],
      "source": [
        "def confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=14):\n",
        "    df_cm = pd.DataFrame(\n",
        "        confusion_matrix, index=class_names, columns=class_names,\n",
        "    )\n",
        "    fig = plt.figure(figsize=figsize)\n",
        "    try:\n",
        "        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n",
        "    except ValueError:\n",
        "        raise ValueError(\"Confusion matrix values must be integers.\")\n",
        "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n",
        "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrmVqxrPMUq6"
      },
      "outputs": [],
      "source": [
        "c_matrix = metrics.confusion_matrix(test_true_labels, test_predicted_labels)\n",
        "df_confusion = pd.crosstab(test_true_labels, test_predicted_labels)\n",
        "df_confusion.to_csv(os.path.join(data_path,\"confusion_matrix.csv\"))\n",
        "\n",
        "confusion_matrix(c_matrix, classes, figsize = (20,7), fontsize=14)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5T2flBwack9"
      },
      "outputs": [],
      "source": [
        "report = metrics.classification_report(test_true_labels, test_predicted_labels, target_names=classes,  output_dict=True)\n",
        "df_report = pd.DataFrame(report).transpose()\n",
        "print(df_report)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}