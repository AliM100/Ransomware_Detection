{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXgZ5xpUHLK3"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOda9uYjQm4R"
      },
      "outputs": [],
      "source": [
        "!git clone 'https://github.com/AliM100/Ransomware_Detection.git'\n",
        "!pip install patool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGtwgKseHJif"
      },
      "outputs": [],
      "source": [
        "!unzip /content/drive/MyDrive/malimg_dataset.zip -d data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3qO82vGtHn6_"
      },
      "outputs": [],
      "source": [
        "!cp -r \"/content/drive/MyDrive/Benign/\" \"/content/data/malimg_paper_dataset_imgs\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "doyaiCPqHGYt"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 19:26:41.967680: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-03-12 19:26:42.356776: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/a/anaconda3/envs/p38/lib/python3.8/site-packages/cv2/../../lib64:\n",
            "2024-03-12 19:26:42.356790: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
            "2024-03-12 19:26:43.394860: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/a/anaconda3/envs/p38/lib/python3.8/site-packages/cv2/../../lib64:\n",
            "2024-03-12 19:26:43.394961: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/a/anaconda3/envs/p38/lib/python3.8/site-packages/cv2/../../lib64:\n",
            "2024-03-12 19:26:43.394969: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import os\n",
        "from math import log\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import shutil\n",
        "import pandas as pd\n",
        "import patoolib\n",
        "import seaborn as sns\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import average_precision_score\n",
        "import tensorflow\n",
        "from keras.applications import ResNet50\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.models import load_model\n",
        "# from keras.preprocessing.image import ImageDataGenerator\n",
        "# from Ransomware_Detection.dataset import load_data,prepare_data\n",
        "from dataset import load_data,prepare_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Tj4ox-SF1Prx"
      },
      "outputs": [],
      "source": [
        "class_index = {'Adialer.C': 0,\n",
        "                'Agent.FYI': 1,\n",
        "                'Allaple.A': 2,\n",
        "                'Allaple.L': 3,\n",
        "                'Alueron.gen!J': 4,\n",
        "                'Autorun.K': 5,\n",
        "                'C2LOP.P': 6,\n",
        "                'C2LOP.gen!g': 7,\n",
        "                'Dialplatform.B': 8,\n",
        "                'Dontovo.A': 9,\n",
        "                'Fakerean': 10,\n",
        "                'Instantaccess': 11,\n",
        "                'Lolyda.AA1': 12,\n",
        "                'Lolyda.AA2': 13,\n",
        "                'Lolyda.AA3': 14,\n",
        "                'Lolyda.AT': 15,\n",
        "                'Malex.gen!J': 16,\n",
        "                'Obfuscator.AD': 17,\n",
        "                'Rbot!gen': 18,\n",
        "                'Skintrim.N': 19,\n",
        "                'Swizzor.gen!E': 20,\n",
        "                'Swizzor.gen!I': 21,\n",
        "                'VB.AT': 22,\n",
        "                'Wintrim.BX': 23,\n",
        "                'Yuner.A': 24,\n",
        "                'Benign':25}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "viGNy9pluoEh"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-12 19:26:45.972969: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2024-03-12 19:26:45.973200: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/a/anaconda3/envs/p38/lib/python3.8/site-packages/cv2/../../lib64:\n",
            "2024-03-12 19:26:45.973288: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/a/anaconda3/envs/p38/lib/python3.8/site-packages/cv2/../../lib64:\n",
            "2024-03-12 19:26:45.973355: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/a/anaconda3/envs/p38/lib/python3.8/site-packages/cv2/../../lib64:\n",
            "2024-03-12 19:26:46.003809: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/a/anaconda3/envs/p38/lib/python3.8/site-packages/cv2/../../lib64:\n",
            "2024-03-12 19:26:46.003884: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/a/anaconda3/envs/p38/lib/python3.8/site-packages/cv2/../../lib64:\n",
            "2024-03-12 19:26:46.003943: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/a/anaconda3/envs/p38/lib/python3.8/site-packages/cv2/../../lib64:\n",
            "2024-03-12 19:26:46.003956: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n",
            "2024-03-12 19:26:46.007913: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "#best one\n",
        "import tensorflow\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten,Input\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.metrics import AUC, Precision, Recall, Accuracy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "lr_schedule = tensorflow.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=0.001,\n",
        "    decay_steps=10000,\n",
        "    decay_rate=0.9)\n",
        "\n",
        "def build_model(target_size_custom, loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=lr_schedule), metrics=['accuracy'],num_classes=26):\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Conv2D(64, kernel_size=(3, 3),\n",
        "                     activation='relu',\n",
        "                     input_shape=(target_size_custom[0],target_size_custom[1],3)))\n",
        "\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "##this try\n",
        "    # model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "    #                  activation='relu',\n",
        "    #                  input_shape=(target_size_custom[0]//2,target_size_custom[1]//2,3)))\n",
        "\n",
        "    # model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "###\n",
        "    # model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "    #                  activation='relu',\n",
        "    #                  input_shape=(target_size_custom[0]//4,target_size_custom[1]//4,3)))\n",
        "\n",
        "    # model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "\n",
        "    # model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(Conv2D(16, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(Flatten())\n",
        "\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(Dense(50, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    model.compile(loss=loss, optimizer = optimizer, metrics=metrics)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten,Input\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.metrics import AUC, Precision, Recall, Accuracy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "lr_schedule = tensorflow.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=0.0001,\n",
        "    decay_steps=10000,\n",
        "    decay_rate=0.9)\n",
        "\n",
        "def build_model(target_size_custom, loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=lr_schedule), metrics=['accuracy'],num_classes=26):\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=(target_size_custom[0], target_size_custom[1], 3)))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Flatten())\n",
        "\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    model.compile(loss=loss, optimizer = optimizer, metrics=metrics)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gpus = tensorflow.config.list_physical_devices('GPU')\n",
        "gpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxKqkHzeHGYu"
      },
      "outputs": [],
      "source": [
        "def train(train_gen,val_gen,target_size_custom,input_shape,save_checkpoints_path,batch_size, epochs):\n",
        "\n",
        "\n",
        "    model = build_model(target_size_custom)\n",
        "\n",
        "    rlrp = ReduceLROnPlateau(monitor='loss', factor=0.2, verbose=1, patience=5, min_lr=0.000001)\n",
        "\n",
        "    history=model.fit(train_gen, validation_data=val_gen, batch_size=batch_size, epochs=epochs, callbacks=[rlrp])\n",
        "\n",
        "\n",
        "    #saving model weights and history\n",
        "    model.save(f'{save_checkpoints_path}/model.h5')\n",
        "\n",
        "    hist_df = pd.DataFrame(history.history)\n",
        "\n",
        "    with open(f\"{save_checkpoints_path}/history.json\", \"w\") as outfile:\n",
        "        hist_df.to_json(outfile)\n",
        "\n",
        "\n",
        "    print(\"***Ploting***\")\n",
        "    epochs = [i for i in range(epochs)]\n",
        "    fig , ax = plt.subplots(1,2)\n",
        "    train_acc = history.history['accuracy']\n",
        "    train_loss = history.history['loss']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    val_loss = history.history['val_loss']\n",
        "\n",
        "\n",
        "    fig.set_size_inches(20,8)\n",
        "    ax[0].plot(epochs , train_loss , label = 'Training Loss')\n",
        "    ax[0].plot(epochs , val_loss , label = 'Testing Loss')\n",
        "    ax[0].set_title('Training & Testing Loss')\n",
        "    ax[0].legend()\n",
        "    ax[0].set_xlabel(\"Epochs\")\n",
        "\n",
        "    ax[1].plot(epochs , train_acc , label = 'Training Accuracy')\n",
        "    ax[1].plot(epochs , val_acc , label = 'Testing Accuracy')\n",
        "    ax[1].set_title('Training & Testing Accuracy')\n",
        "    ax[1].legend()\n",
        "    ax[1].set_xlabel(\"Epochs\")\n",
        "    plt.savefig('train_loss_accuracy.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0qzB956uonp"
      },
      "outputs": [],
      "source": [
        "data_path=\"data\"\n",
        "img_path=\"data/malimg_paper_dataset_imgs\"\n",
        "data_csvs=\"data/csvs\"\n",
        "save_checkpoints_path=\"data/checkpoint\"\n",
        "batch_size=28\n",
        "epochs = 100\n",
        "os.makedirs(save_checkpoints_path,exist_ok=True)\n",
        "os.makedirs(data_csvs,exist_ok=True)\n",
        "\n",
        "data_prepare=prepare_data(data_path,img_path,class_index)\n",
        "\n",
        "if not os.path.exists(f\"{data_csvs}/train.csv\"):\n",
        "    data_prepare.create_csv_data()\n",
        "\n",
        "target_size_custom = (224, 224)\n",
        "input_shape=(224, 224, 3)\n",
        "\n",
        "dataloader=load_data(img_path,data_csvs,target_size_custom,batch_size)\n",
        "train_gen,val_gen=dataloader.train_data()\n",
        "\n",
        "classes = train_gen.class_indices\n",
        "with tensorflow.device('GPU'):\n",
        "  train_gen=train(train_gen,val_gen,target_size_custom,input_shape,save_checkpoints_path,batch_size, epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pRi7FsMHGYv"
      },
      "outputs": [],
      "source": [
        "def test(test_gen,save_checkpoints_path):\n",
        "\n",
        "    model=load_model(f\"{save_checkpoints_path}/model.h5\")\n",
        "\n",
        "    y_pred=model.predict(test_gen)\n",
        "    y_test_predicted = np.argmax(np.array(y_pred),axis = 1)\n",
        "    return y_test_predicted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elPnQszxHGYv"
      },
      "outputs": [],
      "source": [
        "test_gen=dataloader.test_data()\n",
        "y_test_predicted=test(test_gen,save_checkpoints_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kxnpXkDKHrN"
      },
      "outputs": [],
      "source": [
        "y_pred=[]\n",
        "for i in y_test_predicted:\n",
        "  for key,value in class_index.items():\n",
        "    if value==i:\n",
        "      y_pred.append(key)\n",
        "      break\n",
        "print(y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXuoVAWMEjAW"
      },
      "outputs": [],
      "source": [
        "def confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=14):\n",
        "    df_cm = pd.DataFrame(\n",
        "        confusion_matrix, index=class_names, columns=class_names,\n",
        "    )\n",
        "    fig = plt.figure(figsize=figsize)\n",
        "    try:\n",
        "        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n",
        "    except ValueError:\n",
        "        raise ValueError(\"Confusion matrix values must be integers.\")\n",
        "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n",
        "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrmVqxrPMUq6"
      },
      "outputs": [],
      "source": [
        "c_matrix = metrics.confusion_matrix(test_gen.classes, y_test_predicted)\n",
        "df_confusion = pd.crosstab(test_gen.classes, y_test_predicted)\n",
        "df_confusion.to_csv(os.path.join(data_path,\"confusion_matrix.csv\"))\n",
        "\n",
        "confusion_matrix(c_matrix, classes, figsize = (20,7), fontsize=14)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoGj--lVaW6Q"
      },
      "outputs": [],
      "source": [
        "accuracy=metrics.accuracy_score(test_gen.classes, y_test_predicted)\n",
        "print(\"accuracy\",accuracy)\n",
        "\n",
        "IoU=metrics.jaccard_score(test_gen.classes, y_test_predicted,average=\"micro\")\n",
        "f1=metrics.f1_score(test_gen.classes, y_test_predicted,average=\"micro\")\n",
        "print(\"micro IoU\",IoU)\n",
        "print(\"micro f1\",f1)\n",
        "\n",
        "IoU=metrics.jaccard_score(test_gen.classes, y_test_predicted,average=\"macro\")\n",
        "f1=metrics.f1_score(test_gen.classes, y_test_predicted,average=\"macro\")\n",
        "print(\"macro IoU\",IoU)\n",
        "print(\"macro f1\",f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5T2flBwack9"
      },
      "outputs": [],
      "source": [
        "report = metrics.classification_report(test_gen.classes, y_test_predicted, target_names=classes,  output_dict=True)\n",
        "df_report = pd.DataFrame(report).transpose()\n",
        "print(df_report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YzC86zixNY4v"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
